{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatable import make_table_macro\n",
    "from dataprocess import squad_dict2list_batch\n",
    "from model import NetDNN, ODNet, FunctionConcat, BattleNet\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_unit_columns = [4, 73, 74, 75, 76, 77, 83, 141, 311]\n",
    "z_unit_columns = [9, 105, 107, 109, 110, 126, 688]\n",
    "pz_unit_columns = p_unit_columns + z_unit_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = [\"Plain\", \"PlainSlow\", \"BushOne\", \"BushTwo\", \"Corridor3\", \"Ramp3\"]\n",
    "unittypes = [\"pp\", \"zz\", \"zp\"]\n",
    "columns = pz_unit_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_index_p1 = column_index_p2 = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_device = torch.device(\"cuda\")\n",
    "target_dtype = torch.float32\n",
    "pinning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = make_table_macro(r\"E:/output0421/output\", maps, unittypes, pickleprefix=\"pcooa_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooz = make_table_macro(r\"E:/output0421/output\", maps, [\"zz\"], pickleprefix=\"pcooa_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oop = make_table_macro(r\"E:/output0421/output\", maps, [\"pp\"], pickleprefix=\"pcooa_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_index = lambda map_index, unittype_index: unittype_index*len(maps)+map_index\n",
    "oo[table_index(3,2)] # BushTwo, zerg vs protoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalDS(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        assert(len(x)==len(y))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = len(x)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_ssty(tablelet, column_index_p1, column_index_p2):\n",
    "    set_p1 = squad_dict2list_batch(tablelet[(\"setup\", \"squad_p1\")].values, column_index_p1)\n",
    "    set_p2 = squad_dict2list_batch(tablelet[(\"setup\", \"squad_p2\")].values, column_index_p2)\n",
    "    set_battlefield = tablelet[(\"setup\", \"battlefield\")].values\n",
    "    #num = np.unique(set_battlefield, axis=0)\n",
    "    #num = num.shape[0]\n",
    "    num = len(maps)\n",
    "    set_battlefield_onehot = np.eye(num)[set_battlefield]\n",
    "    set_y = tablelet[(\"statistics\", \"winrates\")].values\n",
    "    return set_p1, set_p2, set_battlefield_onehot, set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssty_to_dataset(set_p1, set_p2, set_battlefield_onehot, set_y):\n",
    "    set_x = np.concatenate([set_p1,set_p2, set_battlefield_onehot], axis=1)\n",
    "    tensorx = torch.tensor(set_x, device=target_device, dtype=target_dtype)\n",
    "    tensory = torch.tensor(set_y, device=target_device, dtype=target_dtype)\n",
    "    ds = MinimalDS(tensorx, tensory)\n",
    "    return ds\n",
    "\n",
    "def table_to_dataset(tablelet, column_index_p1, column_index_p2):\n",
    "    return ssty_to_dataset(*table_to_ssty(tablelet, column_index_p1, column_index_p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training & Testing\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn1gen = lambda: NetDNN(len(column_index_p1)+ len(column_index_p2) + len(maps), 48, 48, 48, 24, 1).to(target_device, target_dtype)\n",
    "dnnws1gen = lambda: BattleNet(NetDNN(len(column_index_p1), 24, 24), len(column_index_p1), len(maps), 48 + len(maps), 24, 24, 1).to(target_device, target_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, traindl, testdl, valdl=None, *, iterations=None):\n",
    "    print(net)\n",
    "    bestloss = np.inf\n",
    "    bestnet = None\n",
    "    bestepoch = 0\n",
    "    if iterations is None:\n",
    "        iterations = 50\n",
    "    for i in tqdm(range(iterations), desc=\"epoch\"):\n",
    "        trainer.train_1epoch(traindl, net, criterion, optimizer)\n",
    "        if valdl is not None:\n",
    "            loss = trainer.test_1epoch(valdl, net, criterion)\n",
    "            if bestloss > loss:\n",
    "                bestloss = loss\n",
    "                bestnet = net.state_dict()\n",
    "                bestepoch = i\n",
    "    if valdl is not None:\n",
    "        print(\"best epoch is {} with loss {}\".format(bestepoch, bestloss))\n",
    "    return bestnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dl, net):\n",
    "    acc = trainer.test_1epoch_2args(dl, net, trainer.count_correct_predictions_2args)\n",
    "    loss = trainer.test_1epoch(dl, net, nn.BCEWithLogitsLoss())\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_indexing(table, i, j, offset=0):\n",
    "    i, j = (i+offset)%2000, (j+offset)%2000\n",
    "    return table[i: j] if i < j else pd.concat((table[:j], table[i:]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_loop(cv_i, netgens):\n",
    "    offset = cv_i * 200\n",
    "    # 50, 100, 200, ..., 1600 dataset\n",
    "    traintables = [pd.concat([ring_indexing(table,0,25 * (2**i), offset) for table in oo], axis=0) for i in range(7)]\n",
    "    valtable = pd.concat([ring_indexing(table,1600,1800, offset) for table in oo], axis=0)\n",
    "    testtable = pd.concat([ring_indexing(table,1800,2000, offset) for table in oo], axis=0)\n",
    "    column_index_p1 = column_index_p2 = columns\n",
    "\n",
    "    traindss = [table_to_dataset(traintables[i], column_index_p1, column_index_p2) for i in range(7)]\n",
    "    valds = table_to_dataset(valtable, column_index_p1, column_index_p2)\n",
    "    testds = table_to_dataset(testtable, column_index_p1, column_index_p2)\n",
    "    \n",
    "    traindls = [DataLoader(traindss[i], batch_size=25 * (2**i), shuffle=True, pin_memory=pinning) for i in range(7)]\n",
    "    valdl = DataLoader(valds, batch_size=200, pin_memory=pinning)\n",
    "    testdl = DataLoader(testds, batch_size=200, pin_memory=pinning)\n",
    "    \n",
    "    # generating dataset for each battlefields\n",
    "    testtables_battlefield = [pd.concat([ring_indexing(oo[i*len(maps)+j], 1800,2000, offset) for i in range(len(unittypes))], axis=0) for j in range(0, len(maps))]\n",
    "    testds_battlefield = [table_to_dataset(testtable_, column_index_p1, column_index_p2) for testtable_ in testtables_battlefield]\n",
    "    testdl_battlefield = [DataLoader(ds, batch_size=200) for ds in testds_battlefield]\n",
    "    \n",
    "    outputacc = []\n",
    "    outputloss = []\n",
    "    for i, netgen in enumerate(tqdm(netgens, desc=\"net\")):\n",
    "        for j in tqdm(range(7), desc=\"#data\"):\n",
    "            net = netgen()\n",
    "            accs = []\n",
    "            losses = []\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "            traindl = traindls[j]\n",
    "            bestweight = train(net, criterion, optimizer, traindl, testdl, valdl, iterations=100)\n",
    "            net.load_state_dict(bestweight)\n",
    "            for dl in [traindl, valdl, testdl, *testdl_battlefield]:\n",
    "                acc, loss = test(dl, net)\n",
    "                accs.append(acc)\n",
    "                losses.append(loss)\n",
    "            outputacc.append((i, j, *accs))\n",
    "            outputloss.append((i, j, *losses))\n",
    "    return outputacc, outputloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_loop_plain(cv_i, netgens):\n",
    "    # train only in plain, test in all battlefields.\n",
    "    offset = cv_i * 200\n",
    "    # 50, 100, 200, ..., 1600 dataset\n",
    "    # pick only plain. plain is idx 0\n",
    "    traintables = [pd.concat([ring_indexing(oo[u*len(maps) + 0],0,25 * (2**i), offset) for u in range(len(unittypes))], axis=0) for i in range(7)]\n",
    "    valtable = pd.concat([ring_indexing(oo[u*len(maps) + 0],1600,1800, offset) for u in range(len(unittypes))], axis=0)\n",
    "    testtable = pd.concat([ring_indexing(oo[u*len(maps) + 0],1800,2000, offset) for u in range(len(unittypes))], axis=0)\n",
    "    column_index_p1 = column_index_p2 = columns\n",
    "\n",
    "    traindss = [table_to_dataset(traintables[i], column_index_p1, column_index_p2) for i in range(7)]\n",
    "    valds = table_to_dataset(valtable, column_index_p1, column_index_p2)\n",
    "    testds = table_to_dataset(testtable, column_index_p1, column_index_p2)\n",
    "    \n",
    "    traindls = [DataLoader(traindss[i], batch_size=25 * (2**i), shuffle=True, pin_memory=pinning) for i in range(7)]\n",
    "    valdl = DataLoader(valds, batch_size=200, pin_memory=pinning)\n",
    "    testdl = DataLoader(testds, batch_size=200, pin_memory=pinning)\n",
    "    \n",
    "    # generating dataset for each battlefields\n",
    "    testtables_battlefield = [pd.concat([ring_indexing(oo[i*len(maps)+j], 1800,2000, offset) for i in range(len(unittypes))], axis=0) for j in range(0, len(maps))]\n",
    "    testds_battlefield = [table_to_dataset(testtable_, column_index_p1, column_index_p2) for testtable_ in testtables_battlefield]\n",
    "    testdl_battlefield = [DataLoader(ds, batch_size=200) for ds in testds_battlefield]\n",
    "    \n",
    "    outputacc = []\n",
    "    outputloss = []\n",
    "    for i, netgen in enumerate(tqdm(netgens, desc=\"net\")):\n",
    "        for j in tqdm(range(7), desc=\"#data\"):\n",
    "            net = netgen()\n",
    "            accs = []\n",
    "            losses = []\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "            traindl = traindls[j]\n",
    "            bestweight = train(net, criterion, optimizer, traindl, testdl, valdl, iterations=100)\n",
    "            net.load_state_dict(bestweight)\n",
    "            for dl in [traindl, valdl, testdl, *testdl_battlefield]:\n",
    "                acc, loss = test(dl, net)\n",
    "                accs.append(acc)\n",
    "                losses.append(loss)\n",
    "            outputacc.append((i, j, *accs))\n",
    "            outputloss.append((i, j, *losses))\n",
    "    return outputacc, outputloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Trained with combats on a Plain\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llo1, llo2 = [], []\n",
    "for k in tqdm(range(0,10), desc=\"iteration\"):\n",
    "    lo1, lo2 = [], []\n",
    "    for cv_i in tqdm(range(0,10), desc=\"cross validation\"):\n",
    "        o1, o2 = big_loop_plain(cv_i, [dnn1gen, dnnws1gen])\n",
    "        lo1.append(o1)\n",
    "        lo2.append(o2)\n",
    "    llo1.append(lo1)\n",
    "    llo2.append(lo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print\n",
    "\n",
    "From the table below, the number of training data N is $|N| = 25 \\times 2^\\text{ndata}$.\n",
    "\n",
    "The value from the column net indicates: \n",
    "\n",
    "|Value of Net|Name|\n",
    "|---|--------|\n",
    "| 0 |   DNN  |\n",
    "| 1 | DNN+WS | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Avg. Accuracy\n",
    "o1 = np.average(np.array(llo1), axis=(0,1))\n",
    "pd.DataFrame(o1, columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. Loss\n",
    "o2 = np.average(np.array(llo2), axis=(0,1))\n",
    "pd.DataFrame(o2, columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        pd.DataFrame(llo1[i][j], columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps]).to_csv(\"csvout/acc_plain_b{}_cv{}.csv\".format(i,j))\n",
    "        pd.DataFrame(llo2[i][j], columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps]).to_csv(\"csvout/loss_plain_b{}_cv{}.csv\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "llo1 = [[0]*10 for _ in range(10)]\n",
    "llo2 = [[0]*10 for _ in range(10)]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        llo1[i][j] = pd.read_csv(\"csvout/acc_plain_b{}_cv{}.csv\".format(i,j), index_col=0).values\n",
    "        llo2[i][j] = pd.read_csv(\"csvout/loss_plain_b{}_cv{}.csv\".format(i,j), index_col=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Trained with combats on all battlefields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llo1, llo2 = [], []\n",
    "for k in tqdm(range(0,10), desc=\"iteration\"):\n",
    "    lo1, lo2 = [], []\n",
    "    for cv_i in tqdm(range(0,10), desc=\"cross validation\"):\n",
    "        o1, o2 = big_loop(cv_i, [dnn1gen, dnnws1gen])\n",
    "        lo1.append(o1)\n",
    "        lo2.append(o2)\n",
    "    llo1.append(lo1)\n",
    "    llo2.append(lo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print\n",
    "\n",
    "From the table below, the number of training data N is $|N| = 25 \\times 2^\\text{ndata}$ for each battlefield.\n",
    "\n",
    "The value from the column net indicates: \n",
    "\n",
    "|Value of Net|Name|\n",
    "|---|--------|\n",
    "| 0 |   DNN  |\n",
    "| 1 | DNN+WS | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Avg. Accuracy\n",
    "o1 = np.average(np.array(llo1), axis=(0,1))\n",
    "pd.DataFrame(o1, columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. Loss\n",
    "o2 = np.average(np.array(llo2), axis=(0,1))\n",
    "pd.DataFrame(o2, columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        pd.DataFrame(llo1[i][j], columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps]).to_csv(\"csvout/acc_all_b{}_cv{}.csv\".format(i,j))\n",
    "        pd.DataFrame(llo2[i][j], columns=[\"net\", \"ndata\", \"train\", \"val\", \"test\", *maps]).to_csv(\"csvout/loss_all_b{}_cv{}.csv\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "llo1 = [[0]*10 for _ in range(10)]\n",
    "llo2 = [[0]*10 for _ in range(10)]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        llo1[i][j] = pd.read_csv(\"csvout/acc_all_b{}_cv{}.csv\".format(i,j), index_col=0).values\n",
    "        llo2[i][j] = pd.read_csv(\"csvout/loss_all_b{}_cv{}.csv\".format(i,j), index_col=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Protoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n|c|m|g|f|\n",
    "|:---:|:---|---:|---:|---:|\n",
    "|Probe|84|50|0|1|\n",
    "|Colossus|4|300|200|6|\n",
    "|Zealot|73|100|0|2|\n",
    "|Stalker|74|125|50|2|\n",
    "|HighTemplar|75|50|150|2|\n",
    "|DarkTemplar|76|125|125|2|\n",
    "|Sentry|77|50|100|2|\n",
    "|Immortal|83|250|100|4|\n",
    "|Archon|141|250|250|4|\n",
    "|Adept|311|100|25|2|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cp = np.array([\n",
    " [300,200,6], # 4 colossus\n",
    " [100,0,2],   # 73 zealot\n",
    " [125,50,2],  # 74 stalker\n",
    " [50,100,2],  # 75 ht\n",
    " [125,125,2], # 76 dt\n",
    " [50,100,2],  # 77 sentry\n",
    " [250,100,4], # 83 immortal\n",
    " [250,250,4], # 141 archon\n",
    " [100,25,2]   # 311 adept\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPDPSp = np.array([\n",
    " [350,18.7], # 4 colossus\n",
    " [150,18.6], # 73 zealot\n",
    " [160,9.7],  # 74 stalker\n",
    " [80,3.2],   # 75 ht\n",
    " [120,37.2], # 76 dt\n",
    " [80,8.5],   # 77 sentry\n",
    " [300,19.2], # 83 immortal\n",
    " [360,20.0], # 141 archon\n",
    " [140,6.2]   # 311 adept\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zerg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n|c|m|g|f|\n",
    "|:---:|:---|---:|---:|---:|\n",
    "|Zergling|9|25|0|0.5|\n",
    "|Drone|104|50|0|1|\n",
    "|Queen|105|150|0|2|\n",
    "|Hydralisk|107|100|50|2|\n",
    "|Ultralisk|109|300|200|6|\n",
    "|Roach|110|75|25|2|\n",
    "|Baneling|126|50|25|0.5|\n",
    "|Ravager|688|100|100|3|\n",
    "|Lurker|503|50|100|3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cz = np.array([\n",
    " [25,0,0.5],  # 9 zergling\n",
    " [150,0,2],   # 105 queen\n",
    " [100,50,2],  # 107 hydra\n",
    " [300,200,6], # 109 ultra\n",
    " [75,25,2],   # 110 roach\n",
    " [50,25,0.5], # 126 baneling\n",
    " [100,100,3]  # 688 ravager\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPDPSz = np.array([\n",
    " [35,10.0],   # 9 zergling\n",
    " [175,11.3],  # 105 queen\n",
    " [90,22.2],   # 107 hydra\n",
    " [500,57.4],  # 109 ultra\n",
    " [145,11.2],  # 110 roach\n",
    " [30,20.0],   # 126 baneling\n",
    " [120,14.0]   # 688 ravager\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cpz = np.concatenate((Cp, Cz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPDPSpz = np.concatenate((HPDPSp, HPDPSz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTD1, LTD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.where(x < 0, np.exp(x) / (1 + np.exp(x)), 1 / (1 + np.exp(-x)))\n",
    "\n",
    "def f(s1, s2, g):\n",
    "    return sigmoid(g(s1) - g(s2))\n",
    "\n",
    "def g_ltd(s):\n",
    "    return np.sum(s * HPDPSpz[:,0] * HPDPSpz[:,1], axis=1)\n",
    "\n",
    "def f_ltd(s1, s2):\n",
    "    return f(s1, s2, g_ltd)\n",
    "\n",
    "def g_ltd2(s):\n",
    "    return np.sum(s * np.sqrt(HPDPSpz[:,0]) * HPDPSpz[:,1], axis=1)\n",
    "\n",
    "def f_ltd2(s1, s2):\n",
    "    return f(s1, s2, g_ltd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtable = oo[0]\n",
    "testp1, testp2, testt, testy = table_to_ssty(testtable, column_index_p1, column_index_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ltd = f_ltd(testp1, testp2)\n",
    "o_ltd2 = f_ltd2(testp1, testp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_np(outy,testy):\n",
    "    correct = sum((outy < 0.5) * (testy < 0.5) + (outy > 0.5) * (testy > 0.5))\n",
    "    valid = sum((testy < 0.5) + (testy > 0.5))\n",
    "    return correct/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ltd1\n",
    "benchmark_np(o_ltd, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ltd2\n",
    "benchmark_np(o_ltd2, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Optimizing Unit-Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_logit_1step(rt, s0t, terrain_onehot, boundt, Ct, net, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    at = F.softmax(rt)\n",
    "    st = phi_tensor(at, boundt, Ct)\n",
    "    inp = torch.cat([st, s0t, terrain_onehot])\n",
    "    inp = inp.view(1,-1)\n",
    "    predict = net(inp)\n",
    "    loss = -predict\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sgdlogit(a, a0, terrain, bound, C, net, lr):\n",
    "    boundt = torch.tensor(bound).to(target_device, target_dtype)\n",
    "    Ct = torch.tensor(C).to(target_device, target_dtype)\n",
    "    r = np.log(a)\n",
    "    rt = nn.Parameter(torch.tensor(r, requires_grad=True, device=target_device, dtype=target_dtype))\n",
    "    a0t = torch.tensor(a0).to(target_device, target_dtype)\n",
    "    s0t = phi_tensor(a0t, boundt, Ct).floor()  # floor\n",
    "    num = len(maps)\n",
    "    terrain_onehot = torch.tensor(np.eye(num)[terrain]).to(target_device, target_dtype)\n",
    "    optimizer = optim.SGD([rt], lr=lr)\n",
    "    at = F.softmax(rt)\n",
    "    st = phi_tensor(at, boundt, Ct)\n",
    "    total_iterations = 0\n",
    "    ats = [at]\n",
    "    for i in range(500):\n",
    "        st_ = st.clone().detach()\n",
    "        loss = opt_logit_1step(rt, s0t, terrain_onehot, boundt, Ct, net, optimizer)\n",
    "        #print(loss)\n",
    "        at = F.softmax(rt)\n",
    "        st = phi_tensor(at, boundt, Ct)\n",
    "        ats.append(at)\n",
    "        if (i >= 50) and torch.norm(st_ - st) < 0.1:\n",
    "            total_iterations = i\n",
    "            break\n",
    "    at = F.softmax(rt)\n",
    "    return at.detach().cpu().numpy(), ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_adamlogit(a, a0, terrain, bound, C, net, lr):\n",
    "    boundt = torch.tensor(bound).to(target_device, target_dtype)\n",
    "    Ct = torch.tensor(C).to(target_device, target_dtype)\n",
    "    r = np.log(a)\n",
    "    rt = nn.Parameter(torch.tensor(r, requires_grad=True, device=target_device, dtype=target_dtype))\n",
    "    a0t = torch.tensor(a0).to(target_device, target_dtype)\n",
    "    s0t = phi_tensor(a0t, boundt, Ct).floor()  # floor\n",
    "    num = len(maps)\n",
    "    terrain_onehot = torch.tensor(np.eye(num)[terrain]).to(target_device, target_dtype)\n",
    "    optimizer = optim.Adam([rt], lr=lr)\n",
    "    at = F.softmax(rt)\n",
    "    st = phi_tensor(at, boundt, Ct)\n",
    "    total_iterations = 0\n",
    "    ats = [at]\n",
    "    for i in range(500):\n",
    "        st_ = st.clone().detach()\n",
    "        loss = opt_logit_1step(rt, s0t, terrain_onehot, boundt, Ct, net, optimizer)\n",
    "        #print(loss)\n",
    "        at = F.softmax(rt)\n",
    "        st = phi_tensor(at, boundt, Ct)\n",
    "        ats.append(at)\n",
    "        if (i >= 50) and torch.norm(st_ - st) < 0.1:\n",
    "            total_iterations = i\n",
    "            break\n",
    "    at = F.softmax(rt)\n",
    "    return at.detach().cpu().numpy(), ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_winning(a, a0, terrain, bound, C, net):\n",
    "    num = len(maps)\n",
    "    terrain_onehot = torch.tensor(np.eye(num)[terrain]).to(target_device, target_dtype)\n",
    "    boundt = torch.tensor(bound).to(target_device, target_dtype)\n",
    "    Ct = torch.tensor(C).to(target_device, target_dtype)\n",
    "    at = torch.tensor(a).to(target_device, target_dtype)\n",
    "    a0t = torch.tensor(a0).to(target_device, target_dtype)\n",
    "    s0t = phi_tensor(a0t, boundt, Ct).floor()\n",
    "    st = phi_tensor(at, boundt, Ct).floor()\n",
    "    inp = torch.cat([st, s0t, terrain_onehot])\n",
    "    inp = inp.view(1,-1)\n",
    "    return net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "def phi_tensor(a, b, C):\n",
    "    D = torch.matmul(a.reshape((-1,1)), b.reshape((1,-1)))\n",
    "    E = torch.div(D, C + eps)\n",
    "    s,_ = E.min(axis=1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_a(dim):\n",
    "    return np.random.dirichlet([1]*dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_armies(n_iterations):\n",
    "    enemy_as = []\n",
    "    ally_as = []\n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        enemy_a = get_random_a(unitdim)\n",
    "        ally_a = get_random_a(unitdim)\n",
    "        enemy_as.append(enemy_a)\n",
    "        ally_as.append(ally_a)\n",
    "    return enemy_as, ally_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_armies(ally_as, enemy_as, terrain_list, bound, C, objective_net):\n",
    "    optimized_as = []\n",
    "    valss = []\n",
    "    iterations = []\n",
    "    assert(len(enemy_as) == len(ally_as))\n",
    "    assert(len(terrain_list) == len(ally_as))\n",
    "    for i in tqdm(range(len(enemy_as))):\n",
    "        niter = 500\n",
    "        inter_a_s = [get_random_a(unitdim) for _ in range(niter)]\n",
    "        inter_a_s[0] = ally_as[i]\n",
    "        enemy_a = enemy_as[i]\n",
    "        terrain = terrain_list[i]\n",
    "        vals = [eval_winning(a, enemy_a, terrain, bound, C, objective_net).item() for a in inter_a_s]\n",
    "        optimized_a = inter_a_s[np.argmax(vals)]\n",
    "        optimized_as.append(optimized_a)\n",
    "        valss.append(vals)\n",
    "        iterations.append(len(inter_a_s))\n",
    "    return optimized_as, valss, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_armies(optimize_fn, ally_as, enemy_as, terrain_list, bound, C, objective_net, lr):\n",
    "    optimized_as = []\n",
    "    valss = []\n",
    "    iterations = []\n",
    "    assert(len(enemy_as) == len(ally_as))\n",
    "    assert(len(terrain_list) == len(ally_as))\n",
    "    for i in tqdm(range(len(enemy_as))):\n",
    "        enemy_a = enemy_as[i]\n",
    "        ally_a = ally_as[i]\n",
    "        terrain = terrain_list[i]\n",
    "        optimized_a, inter_a_s = optimize_fn(ally_a, enemy_a, terrain, bound, C, objective_net, lr)\n",
    "        vals = [eval_winning(a, enemy_a, terrain, bound, C, objective_net).item() for a in inter_a_s]\n",
    "        optimized_as.append(optimized_a)\n",
    "        valss.append(vals)\n",
    "        iterations.append(len(inter_a_s))\n",
    "    return optimized_as, valss, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment settings for Protoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myC = Cp\n",
    "unitdim = 9\n",
    "column_index_p1 = column_index_p2 = p_unit_columns\n",
    "target_table = oop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment settings for Zerg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myC = Cz\n",
    "unitdim = 7\n",
    "column_index_p1 = column_index_p2 = z_unit_columns\n",
    "target_table = ooz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traintable = pd.concat([table[0:1600] for table in target_table], axis=0)\n",
    "valtable = pd.concat([table[1600:1800] for table in target_table], axis=0)\n",
    "testtable = pd.concat([table[1800:2000] for table in target_table], axis=0)\n",
    "trainds = table_to_dataset(traintable, column_index_p1, column_index_p2)\n",
    "valds = table_to_dataset(valtable, column_index_p1, column_index_p2)\n",
    "testds = table_to_dataset(testtable, column_index_p1, column_index_p2)\n",
    "traindl = DataLoader(trainds, batch_size=200, shuffle=True,  pin_memory=pinning)\n",
    "valdl = DataLoader(valds, batch_size=200, pin_memory=pinning)\n",
    "testdl = DataLoader(testds, batch_size=200, pin_memory=pinning)\n",
    "\n",
    "trainer.verbose = False\n",
    "dnnws = BattleNet(NetDNN(len(column_index_p1), 24, 24), len(column_index_p1), len(maps), 48 + len(maps), 24, 24, 1).to(target_device, target_dtype)\n",
    "net = dnnws\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "bestweight = train(net, criterion, optimizer, traindl, testdl, valdl)\n",
    "net.load_state_dict(bestweight)\n",
    "loss = test(testdl, net)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource bound\n",
    "bound = np.array([10000, 5000, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundt = torch.tensor(bound).to(target_device, target_dtype)\n",
    "Ct = torch.tensor(myC).to(target_device, target_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### squad and terrain initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_list = np.array([0,1,2,3,4,5] * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enemy_as, ally_as = initialize_armies(1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mc\n",
    "_, randvalss, _ = random_armies(ally_as, enemy_as, terrain_list, bound, myC, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "sgd1_optimized_as, sgd1_valss, sgd1_iterations = optimize_armies(optimize_sgdlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 1)\n",
    "sgd01_optimized_as, sgd01_valss, sgd01_iterations = optimize_armies(optimize_sgdlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 0.1)\n",
    "sgd001_optimized_as, sgd001_valss, sgd001_iterations = optimize_armies(optimize_sgdlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 0.01)\n",
    "adam1_optimized_as, adam1_valss, adam1_iterations = optimize_armies(optimize_adamlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 1)\n",
    "adam01_optimized_as, adam01_valss, adam01_iterations = optimize_armies(optimize_adamlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 0.1)\n",
    "adam001_optimized_as, adam001_valss, adam001_iterations = optimize_armies(optimize_adamlogit, ally_as, enemy_as, terrain_list, bound, myC, net, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.save(\"save/sgd1_all_pp\",sgd1_valss)\n",
    "np.save(\"save/sgd01_all_pp\",sgd01_valss)\n",
    "np.save(\"save/sgd001_all_pp\",sgd001_valss)\n",
    "np.save(\"save/adam1_all_pp\",adam1_valss)\n",
    "np.save(\"save/adam01_all_pp\",adam01_valss)\n",
    "np.save(\"save/adam001_all_pp\",adam001_valss)\n",
    "np.save(\"save/mc_all_pp\",randvalss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### result plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printavgminmax = lambda l0 : print(np.average(l0), np.min(l0), np.max(l0))\n",
    "printavgminmaxll = lambda ll : print(np.average(ll, axis=0), np.min(ll, axis=0), np.max(ll, axis=0))\n",
    "\n",
    "def getmaxsofar(li, match_length=500):\n",
    "    \"\"\"For MC, calculate the max value in li[:i] for each step i.\"\"\"\n",
    "    li2 = []\n",
    "    maxsofar = -np.inf\n",
    "    for i in li:\n",
    "        if maxsofar < i:\n",
    "            maxsofar = i\n",
    "        li2.append(maxsofar)\n",
    "    # elongate\n",
    "    if len(li2) < match_length:\n",
    "        li2.extend([maxsofar] * (match_length - len(li2)))\n",
    "    # cut\n",
    "    return li2[:match_length]\n",
    "\n",
    "def fillmax(li, match_length=500):\n",
    "    \"\"\"For gradient updates, if converged at step i (i<match_length),\n",
    "    then fill max values in li[i:match_length].\"\"\"\n",
    "    li2 = li[:]\n",
    "    # elongate\n",
    "    if len(li2) < match_length:\n",
    "        li2.extend([li[-1]] * (match_length - len(li2)))\n",
    "    # cut\n",
    "    return li2[:match_length]\n",
    "\n",
    "printavgminmax(sgd001_iterations)\n",
    "printavgminmax(adam1_iterations)\n",
    "#printavgminmax(sgd001_optimized_as)\n",
    "sgd001_valss_max = [fillmax(sgd001_vals) for sgd001_vals in sgd001_valss]\n",
    "sgd01_valss_max = [fillmax(sgd01_vals) for sgd01_vals in sgd01_valss]\n",
    "sgd1_valss_max = [fillmax(sgd1_vals) for sgd1_vals in sgd1_valss]\n",
    "adam001_valss_max = [fillmax(adam001_vals) for adam001_vals in adam001_valss]\n",
    "adam01_valss_max = [fillmax(adam01_vals) for adam01_vals in adam01_valss]\n",
    "adam1_valss_max = [fillmax(adam1_vals) for adam1_vals in adam1_valss]\n",
    "#printavgminmaxll(gdvalss_max)\n",
    "#printavgminmax(max_as)\n",
    "randvalss_max = [getmaxsofar(randvals) for randvals in randvalss]\n",
    "#printavgminmaxll(randvalss_max)\n",
    "plt.plot(range(500), np.average(sgd001_valss_max, axis=0), label=\"sgd 0.01\")\n",
    "plt.plot(range(500), np.average(sgd01_valss_max, axis=0), label=\"sgd 0.1\")\n",
    "plt.plot(range(500), np.average(sgd1_valss_max, axis=0), label=\"sgd 1\")\n",
    "plt.plot(range(500), np.average(adam001_valss_max, axis=0), label=\"adam 0.01\")\n",
    "plt.plot(range(500), np.average(adam01_valss_max, axis=0), label=\"adam 0.1\")\n",
    "plt.plot(range(500), np.average(adam1_valss_max, axis=0), label=\"adam 1\")\n",
    "plt.plot(range(500), np.average(randvalss_max, axis=0), label=\"mc\")\n",
    "plt.ylabel(\"average logit(y)\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(plt.rcParams[\"figure.figsize\"][0] *2, plt.rcParams[\"figure.figsize\"][1]*2))\n",
    "for ax in axs[1]:\n",
    "    #ax.plot(range(500), np.average(sigmoid(np.array(sgd001_valss_max)), axis=0), label=\"sgd 0.01\")\n",
    "    #ax.plot(range(500), np.average(sigmoid(np.array(sgd01_valss_max)), axis=0), label=\"sgd 0.1\")\n",
    "    ax.plot(range(500), np.average(sigmoid(np.array(sgd1_valss_max)), axis=0), label=\"sgd 1\")\n",
    "    ax.plot(range(500), np.average(sigmoid(np.array(adam001_valss_max)), axis=0), label=\"adam 0.01\")\n",
    "    ax.plot(range(500), np.average(sigmoid(np.array(adam01_valss_max)), axis=0), label=\"adam 0.1\")\n",
    "    ax.plot(range(500), np.average(sigmoid(np.array(adam1_valss_max)), axis=0), label=\"adam 1\")\n",
    "    ax.plot(range(500), np.average(sigmoid(np.array(randvalss_max)), axis=0), label=\"mc\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_xlabel(\"steps\")\n",
    "    ax.legend()\n",
    "for ax in axs[0]:\n",
    "    #ax.plot(range(500), np.average(sgd001_valss_max, axis=0), label=\"sgd 0.01\")\n",
    "    #ax.plot(range(500), np.average(sgd01_valss_max, axis=0), label=\"sgd 0.1\")\n",
    "    ax.plot(range(500), np.average(sgd1_valss_max, axis=0), label=\"sgd 1\")\n",
    "    ax.plot(range(500), np.average(adam001_valss_max, axis=0), label=\"adam 0.01\")\n",
    "    ax.plot(range(500), np.average(adam01_valss_max, axis=0), label=\"adam 0.1\")\n",
    "    ax.plot(range(500), np.average(adam1_valss_max, axis=0), label=\"adam 1\")\n",
    "    ax.plot(range(500), np.average(randvalss_max, axis=0), label=\"mc\")\n",
    "    ax.set_ylabel(\"logit(y)\")\n",
    "    ax.set_xlabel(\"steps\")\n",
    "    ax.legend()\n",
    "axs[0][1].set_xlim(0,100)\n",
    "axs[1][1].set_xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
